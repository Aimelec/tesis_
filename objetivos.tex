\chapter{Objetivos}

El presente trabajo se inserta dentro de una línea de investigación llevada adelante por el equipo del Laboratorio de Inteligencia Artificial Aplicada (Juan Kamienkowski, Bruno Bianchi, y Fermín Travi). El objetivo general de esta línea de trabajo es el de mejorar modelos del campo del NLP a partir de adicionarles información cognitiva a los mismos durante el entrenamiento.

Para avanzar en línea con ese objetivo general, la presente tesis se propone los siguientes objetivos particulares:
\begin{itemize}
    \item Preprocesar datos de movimientos oculares: Para el cual se realizaron una serie de experimentos registrando los movimientos oculares de los sujetos mientras realizaban tareas de lectura sobre una serie seleccionada de textos breves de fuentes literarias e informativas. A partir de estos resultados se buscará acondicionar los mismos para ser utilizados en el entrenamiento de los modelos de lenguaje.
    \item Seleccionar un modelo de lenguaje pertinente a la tarea a realizar: Partiendo en este caso de la arquitectura AWD-LSTM, se buscará adaptarla a la tarea de modelado de lenguaje a partir de los datos de movimientos oculares, ademas de realizarle distintas modificaciones para mejorar su rendimiento en diversos aspectos.
    \item Realizar diferentes entrenamientos del modelo de lenguaje elegido: Generando a partir de esta arquitectura ya adaptada a nuestras necesidades distintos modelos de lenguaje, ajustando los hiperparámetros y las condiciones de entrenamiento para obtener los mejores resultados posibles y asi obtener un modelo base con el cual trabajar.
    \item Analizar los modelos obtenidos: Comparando los \textit{embeddings} obtenidos a partir de los modelos de lenguaje reentrenados con los movimientos oculares con distintos repositorios de juicios de similitud humanos, con el objetivo de observar que tan bien estos capturan las relaciones entre ciertas palabras. 
\end{itemize}

% El estudio del lenguaje ha sido, históricamente, un punto de convergencia entre distintas disciplinas como la lingüística, la neurociencia y la computación. En el caso de la lingüística, aspirando a comprender cómo se estructuran los idiomas y el lenguaje. La neurociencia por otro lado, enfocándose en comprender los procesos cerebrales que subyacen a esta capacidad. Y por último la computación, que desde sus comienzos a mediados del siglo pasado ha buscado tanto la forma de emular la comunicación entre humanos, como de lograr una comunicación fluida entre los humanos y las máquinas (\cite{Shannon1948}, \cite{Turing1950}). Esta última disciplina ha logrado grandes avances en sus objetivos en los últimos años mediante modelos de Aprendizaje Profundo, con capacidades lingüísticas similares a los humanos \parencite{ruder2018nlps}. El comportamiento de estos modelos genera una serie de preguntas sobre sus mecanismos internos. En particular, ¿son los mecanismos internos de estos modelos análogos a los que ocurren en el cerebro humano? ¿Sería posible optimizar su entrenamiento y su funcionamiento adicionándoles información cognitiva?

% Estos modelos logran una gran capacidad de abstracción a partir de ser entrenados con grandes volúmenes de datos, optimizando una enorme cantidad de parámetros internos. Por ejemplo, los modelos del estado del arte superan los cientos de miles de millones de parámetros y son entrenados y evaluados con cientos de \textit{gigabytes} de textos, en su gran mayoría en inglés. \parencite{touvron2023llama2openfoundation} Esto implica un gran costo computacional y económico, haciendo prácticamente imposible entrenar un modelo con un comportamiento similar a los comerciales (en algunos casos cerrados, y sin acceso completo a las activaciones internas) por parte de grupos de investigación independientes. Más aún, la evaluación de las capacidades de comprensión de estos modelos está fuertemente limitada al idioma inglés (\cite{bisk2019piqareasoningphysicalcommonsense}, \cite{mihaylov2018suitarmorconductelectricity}, \cite{rajpurkar2018knowdontknowunanswerable}), con unos pocos conjuntos de datos en español que son traducciones directas de los primeros, manteniendo los sesgos culturales que los caracterizan.

% Desde el área de la Neurociencia Cognitiva Computacional, se plantea que es posible estudiar y comprender el funcionamiento de estos modelos de forma análoga a como se estudia el lenguaje en el cerebro. En ambos casos nos estamos enfrentando a una “caja negra”, a la cual le podemos introducir valores y obtener respuestas, o medir la actividad interna en distintas etapas del procesamiento. (\cite{abnar2019blackboxmeetsblackboxrepresentational}, \cite{toneva2019}) En éste camino existe un campo de investigación naciente en la interfaz entre la Neurociencia y la Inteligencia Artificial, que se propone extraer información de las redes neuronales que sea útil para comprender el funcionamiento del cerebro, y viceversa.

% El presente trabajo propone aportar a las investigaciones actualmente efectuadas por el LIAA, las cuales buscan embarcarse en este campo novedoso y disruptivo, con el objetivo principal de obtener modelos de lenguaje que cuenten con mayor información cognitiva en español. En particular, en esta tesis se buscará analizar un modelo de lenguaje basado en LSTMs, para el cual usaremos un modelo del estado del arte en lo que a LSTM respecta, llamado AWD-LSTM. A partir de esto, se indagará sobre el comportamiento del mismo al añadirle la nueva información cognitiva, comparando los \textit{embeddings} del modelo con juicios de similitud humanos, con la idea de observar que tan bien estos capturan las relaciones entre ciertas palabras. Además, también se comparará este modelo con otros que se han estado investigando en el laboratorio, como Word2Vec.
