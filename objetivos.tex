\chapter{Objetivos}

El estudio del lenguaje ha sido, históricamente, un punto de convergencia entre distintas disciplinas como la lingüística, la neurociencia y la computación. En el caso de la lingüística, aspirando a comprender cómo se estructuran los idiomas y el lenguaje. La neurociencia por otro lado, enfocándose en comprender los procesos cerebrales que subyacen a esta capacidad. Y por último la computación, que desde sus comienzos a mediados del siglo pasado ha buscado tanto la forma de emular la comunicación entre humanos, como de lograr una comunicación fluida entre los humanos y las máquinas (\cite{Shannon1948}, \cite{Turing1950}). Esta última disciplina ha logrado grandes avances en sus objetivos en los últimos años mediante modelos de Aprendizaje Profundo, con capacidades lingüísticas similares a los humanos \parencite{ruder2018nlps}. El comportamiento de estos modelos genera una serie de preguntas sobre sus mecanismos internos. En particular, ¿son los mecanismos internos de estos modelos análogos a los que ocurren en el cerebro humano? ¿Sería posible optimizar su entrenamiento y su funcionamiento adicionándoles información cognitiva?

Estos modelos logran una gran capacidad de abstracción a partir de ser entrenados con grandes volúmenes de datos, optimizando una enorme cantidad de parámetros internos. Por ejemplo, los modelos del estado del arte superan los cientos de miles de millones de parámetros y son entrenados y evaluados con cientos de \textit{gigabytes} de textos, en su gran mayoría en inglés. \parencite{touvron2023llama2openfoundation} Esto implica un gran costo computacional y económico, haciendo prácticamente imposible entrenar un modelo con un comportamiento similar a los comerciales (en algunos casos cerrados, y sin acceso completo a las activaciones internas) por parte de grupos de investigación independientes. Más aún, la evaluación de las capacidades de comprensión de estos modelos está fuertemente limitada al idioma inglés (\cite{bisk2019piqareasoningphysicalcommonsense}, \cite{mihaylov2018suitarmorconductelectricity}, \cite{rajpurkar2018knowdontknowunanswerable}), con unos pocos conjuntos de datos en español que son traducciones directas de los primeros, manteniendo los sesgos culturales que los caracterizan.

Desde el área de la Neurociencia Cognitiva Computacional, se plantea que es posible estudiar y comprender el funcionamiento de estos modelos de forma análoga a como se estudia el lenguaje en el cerebro. En ambos casos nos estamos enfrentando a una “caja negra”, a la cual le podemos introducir valores y obtener respuestas, o medir la actividad interna en distintas etapas del procesamiento. (\cite{abnar2019blackboxmeetsblackboxrepresentational}, \cite{toneva2019}) En éste camino existe un campo de investigación naciente en la interfaz entre la Neurociencia y la Inteligencia Artificial, que se propone extraer información de las redes neuronales que sea útil para comprender el funcionamiento del cerebro, y viceversa.

El presente trabajo propone aportar a las investigaciones actualmente efectuadas por el LIAA, las cuales buscan embarcarse en este campo novedoso y disruptivo, con el objetivo principal de obtener modelos de lenguaje que cuenten con mayor información cognitiva en español. En particular, en esta tesis se buscará analizar un modelo de lenguaje basado en LSTMs, para el cual usaremos un modelo del estado del arte en lo que a LSTM respecta, llamado AWD-LSTM. A partir de esto, se indagará sobre el comportamiento del mismo al añadirle la nueva información cognitiva, comparando los \textit{embeddings} del modelo con juicios de similitud humanos, con la idea de observar que tan bien estos capturan las relaciones entre ciertas palabras. Además, también se comparará este modelo con otros que se han estado investigando en el laboratorio, como Word2Vec.
